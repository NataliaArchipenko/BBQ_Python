{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import interact\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://databasecamp.de/ki/gradientenverfahren-ein-steiler-abstieg?paged312=3#was-ist-die-grundidee-von-gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Der Gradient – Was ist das eigentlich?\n",
    "\n",
    "\n",
    "Stell dir eine Landschaft vor, die Hügel und Täler hat. Du stehst irgendwo auf dieser Landschaft und möchtest ins tiefste Tal (das Minimum der Zielfunktion).\n",
    "\n",
    "Der **Gradient** ist wie ein Kompass, der dir zeigt:\n",
    "- **In welche Richtung** die Landschaft am steilsten ansteigt (der steilste Bergaufweg).\n",
    "- **Wie steil** dieser Anstieg ist.\n",
    "\n",
    "Um ins Tal zu gelangen, gehst du in die **entgegengesetzte Richtung des Gradienten** – das ist der steilste Weg nach unten.\n",
    "\n",
    "## Anschauliche Bedeutung des Gradienten\n",
    "\n",
    "### Einfach gesagt:\n",
    "- Der Gradient ist wie ein **\"Gefälle-Messer\"**. Er misst die Steigung der Zielfunktion in Bezug auf eine oder mehrere Variablen.\n",
    "- **Wenn der Gradient groß ist**, bedeutet das, dass die Steigung steil ist.\n",
    "- **Wenn der Gradient klein ist**, ist die Steigung flach – das bedeutet, du bist nahe an einem Extremwert bzw. Sattelpunkt/Wendepunkt.\n",
    "\n",
    "### Mathematisch:\n",
    "- Der Gradient ist ein **Vektor**, der die Richtung des stärksten Anstiegs einer Funktion zeigt.\n",
    "- Wenn die Zielfunktion $J(m, b)$ von zwei Variablen ($m$) und ($b$)) abhängt, gibt der Gradient die Richtung an, in die sich $J$ am stärksten verändert:\n",
    "  $$\n",
    "  \\nabla J(m, b) = \\left( \\frac{\\partial J}{\\partial m}, \\frac{\\partial J}{\\partial b} \\right)\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "## Beispiel mit einer Steigung\n",
    "\n",
    "- Stelle dir eine **Bergkuppe** vor. Wenn du dort stehst, spürst du, dass es in eine bestimmte Richtung steil bergab geht (das ist der negative Gradient).\n",
    "- Der Gradient zeigt aber immer **nach oben**, in Richtung des steilsten Anstiegs.\n",
    "\n",
    "\n",
    "\n",
    "## Gradient in der linearen Regression\n",
    "\n",
    "In der linearen Regression möchten wir die Parameter $m$ (Steigung) und $b$ (Achsenabschnitt) so optimieren, dass der Fehler (Zielfunktion) minimiert wird. Der Gradient hilft uns dabei:\n",
    "\n",
    "1. **Richtung und Veränderung:**\n",
    "   - Er zeigt uns, wie sich der Fehler $J(m, b)$ verändert, wenn wir $m$ oder $b$ leicht anpassen.\n",
    "   - Er gibt die Richtung an, in die wir die Parameter anpassen sollten, um $J(m, b)$ kleiner zu machen.\n",
    "\n",
    "2. **Beispiel für $m$:**\n",
    "   - Wenn der Gradient für $m$ positiv ist ($\\frac{\\partial J}{\\partial m} > 0$):\n",
    "     - Das bedeutet, dass der Fehler größer wird, wenn wir $m$ erhöhen.\n",
    "     - Daher verringern wir $m$, um den Fehler zu minimieren.\n",
    "\n",
    "3. **Beispiel für $b$:**\n",
    "   - Wenn der Gradient für $b$ negativ ist ($\\frac{\\partial J}{\\partial b} < 0$):\n",
    "     - Das bedeutet, dass der Fehler kleiner wird, wenn wir $b$ erhöhen.\n",
    "     - Daher erhöhen wir $b$, um den Fehler zu minimieren.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ziel- bzw. Verlustfunktion\n",
    "Die Zielfunktion misst die Qualität des Modells, indem sie die Abweichung zwischen den tatsächlichen Werten\n",
    "und den vorhergesagten Werten berechnet. Dies erfolgt über die Summe der kleinsten Quadrate, die auch als Mean Squared Error (MSE) bezeichnet wird.\n",
    "\n",
    "Eigenschaften der Zielfunktion:\n",
    " - Differenzierbar:\n",
    "    Die Zielfunktion ist glatt und stetig, was bedeutet, dass wir ihre Ableitungen berechnen können. Dies ist eine wichtige Voraussetzung für den Gradientenabstieg.\n",
    "\n",
    "- Konvex:\n",
    "    Da die Zielfunktion quadratisch ist, besitzt sie genau ein globales Minimum. Das garantiert, dass der Gradientenabstieg immer den besten Wert findet, wenn die Lernrate korrekt gewählt ist.\n",
    "\n",
    "- Bewertung der Modellqualität:\n",
    "    Die Zielfunktion misst präzise, wie gut das Modell die Daten beschreibt. Ein niedriger Wert bedeutet, dass die Abweichungen zwischen den Vorhersagen und den tatsächlichen Werten gering sind.\n",
    "\n",
    "- Ziel des Gradientenverfahrens:\n",
    "    Das Ziel des Gradientenabstiegs ist es, die Zielfunktion zu minimieren. Das bedeutet:\n",
    "\n",
    "    Wir passen die Modellparameter so an, dass die Zielfunktion ihren kleinsten Wert erreicht.\n",
    "    An diesem Punkt sind die Abstände zwischen den Vorhersagen und den tatsächlichen Werten minimal, und unser Modell beschreibt die Daten optimal.\n",
    "\n",
    "Da die Zielfunktion quadratisch ist, hat sie die Form einer \"Schüssel\". Das globale Minimum ist der tiefste Punkt dieser Schüssel. Der Gradientenabstieg hilft uns, diesen Punkt zu finden, indem er die Steigung der Funktion (den \"Gradienten\") verwendet, um die Parameter schrittweise zu optimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(intercept, coef, x, y):\n",
    "    total_cost = 0\n",
    "    N = len(x)\n",
    "    for i in range(N):\n",
    "        total_cost += (y[i] - (coef * x[i] + intercept)) ** 2\n",
    "    return total_cost / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grafische Darstellung einer quadratischen Funktion \n",
    "**Bei Darstellungsfehlern Code Zelle nochmal ausführen !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "def f(x, A, B, C):\n",
    "    return A * x**2 + B * x + C\n",
    "\n",
    "def f_derivative(x, A, B):\n",
    "    return 2 * A * x + B\n",
    "\n",
    "def update(A=1, B=0, C=0):\n",
    "    \n",
    "    ax.clear()\n",
    "    y = f(x, A, B, C)\n",
    "    ax.plot(x, y, label=f\"$f(x) = {A:.2f}\\\\times x^2 + {B:.2f}x + {C:.2f}$\")\n",
    "\n",
    "    if A != 0:\n",
    "        # Berechne Extrempunkt\n",
    "        x_extreme = -B / (2 * A)\n",
    "        y_extreme = f(x_extreme, A, B, C)\n",
    "        tangent_slope = f_derivative(x_extreme, A, B)\n",
    "        tangent = tangent_slope * (x - x_extreme) + y_extreme\n",
    "\n",
    "        # Plot Extrempunkt und Tangente\n",
    "        ax.plot(x_extreme, y_extreme, 'ro', label=f\"Extrempunkt ({x_extreme:.2f}, {y_extreme:.2f})\")\n",
    "        ax.plot(x, tangent, 'g--', label=\"Tangente am Extrempunkt\")\n",
    "    \n",
    "    \n",
    "    ax.axhline(color=\"gray\", linestyle=\"--\", alpha=.25)\n",
    "    ax.axvline(color=\"gray\", linestyle=\"--\", alpha=.25)\n",
    "\n",
    "    ax.set_title(\"Interaktives Diagramm mit Extrempunkt und Tangente\")\n",
    "    ax.set_ylim(-25, 45)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"f(x)\")\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Interaktives Widget erstellen\n",
    "fig, ax = plt.subplots()\n",
    "interact(update, A=(-4, 4, 0.1), B=(-10, 10, 0.1), C=(-20, 40, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grafische Dartsellung der Regression inklusive MSE (Mean squared Error = Summe der (kleinsten) (Fehler-) Quadrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 50)\n",
    "Y = 2.5 * (X + 1) + np.random.normal(0, 2, len(X))\n",
    "\n",
    "# Update-Funktion für die interaktive Darstellung\n",
    "def update(slope=0.0, intercept=0.0):\n",
    "    ax.clear()\n",
    "    ax.set_ylim(-2, 30)\n",
    "    # Vorhergesagte Werte basierend auf der aktuellen Geraden\n",
    "    Y_pred = slope * X + intercept\n",
    "    \n",
    "    # Berechnung des Fehlers\n",
    "    mse = compute_cost(intercept, slope, X, Y)\n",
    "    \n",
    "    # Plotte die Datenpunkte\n",
    "    ax.scatter(X, Y, color='blue', label='Datenpunkte')\n",
    "    \n",
    "    # Plotte die Regressionsgerade\n",
    "    ax.plot(X, Y_pred, color='red', label=f'Regressionsgerade\\nMSE: {mse:.2f}')\n",
    "    \n",
    "    \n",
    "    ax.set_title(\"Interaktive lineare Regression\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.legend()\n",
    "\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "interact(update, slope=(-25.0, 25.0, 0.1), intercept=(-50.0, 50.0, 0.1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimierung - der Gradientenabstieg\n",
    "Die Funktion step_gradient implementiert einen Schritt des Gradientenabstiegs für die lineare Regression.<br> \n",
    "Sie berechnet die Gradienten (Ableitungen) der Zielfunktion in Bezug auf die Parameter intercept (Achsenabschnitt) und coef (Steigung) <br> \n",
    "und aktualisiert diese Parameter basierend auf der Lernrate (learning_rate).<br>\n",
    "\n",
    "<img src=\"https://databasecamp.de/wp-content/uploads/gradient-descent-example-1-1024x805.png\" width=\"800\"></img>\n",
    "<img src=\"https://www.statworx.com/wp-content/uploads/learning-rate.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Berechnung der Gradienten und Anpassung der Modellparameter\n",
    "Dieser Code implementiert einen Schritt des Gradientenabstiegs, einer Optimierungsmethode, die darauf abzielt, die Parameter einer Funktion so anzupassen, dass der Fehler zwischen den Vorhersagen der Funktion und den tatsächlichen Zielwerten minimiert wird. Konkret werden hier die Parameter einer linearen Funktion, nämlich der Achsenabschnitt (intercept_current) und die Steigung (coef_current), optimiert.\n",
    "\n",
    "Sobald die Gradienten für die Steigung und den Achsenabschnitt berechnet sind, werden die Parameter aktualisiert. Dabei werden kleine Schritte in Richtung des steilsten Abstiegs des Fehlers unternommen. Die Schrittweite bzw. Lernrate, die angibt, wie groß diese Anpassungen sein sollen, wird durch den Wert des learning_rate bestimmt.\n",
    "\n",
    "Am Ende des Prozesses gibt der Code die aktualisierten Werte für die Steigung (coef_updated) und den Achsenabschnitt (intercept_updated) zurück. Diese neuen Werte werden in weiteren Iterationen verwendet, um die Funktion schrittweise weiter zu verbessern. So nähert sich die Funktion mit jedem Schritt den optimalen Parametern, die den Fehler minimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(intercept_current, coef_current, x, y, learning_rate):\n",
    "    coef_gradient = 0\n",
    "    intercept_gradient = 0\n",
    "    N = len(x)\n",
    "    for i in range(N):\n",
    "        coef_gradient += - (2/N) * x[i] * (y[i] - (coef_current * x[i] + intercept_current))\n",
    "        intercept_gradient += - (2/N) * (y[i] - (coef_current * x[i] + intercept_current))\n",
    "    coef_updated = coef_current - learning_rate * coef_gradient\n",
    "    intercept_updated = intercept_current - learning_rate * intercept_gradient\n",
    "    return intercept_updated, coef_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterativer Optimierungsprozess mit Gradientenabstieg zur Minimierung der Fehlerfunktion\n",
    "Die Funktion `gradient_descent_runner` führt das Gradientenabstiegsverfahren aus, um die Parameter eines linearen Modells, nämlich den Achsenabschnitt (`intercept`) und die Steigung (`coef`), zu optimieren. Ziel ist es, die Werte so anzupassen, dass der Fehler zwischen den tatsächlichen Zielwerten (`y`) und den Vorhersagen des Modells minimiert wird. Das Verfahren erfolgt iterativ über eine festgelegte Anzahl von Durchläufen.\n",
    "\n",
    "Zu Beginn werden die Startwerte für die Parameter `intercept` und `coef` gesetzt, die als Ausgangspunkt des Optimierungsprozesses dienen. Zusätzlich werden leere Listen initialisiert, um den Verlauf der Kostenfunktion (`cost_graph`) sowie die Änderungen der Parameter über die Iterationen hinweg (`path_intercept` und `path_coef`) zu speichern.\n",
    "\n",
    "In einer Schleife, die über die angegebene Anzahl von Iterationen (`num_iterations`) läuft, wird zunächst der Fehler für die aktuellen Parameter berechnet. Dieser Fehler, der die Güte des Modells misst, wird in der Liste `cost_graph` gespeichert. Gleichzeitig werden die aktuellen Werte der Parameter `intercept` und `coef` in den Pfadlisten `path_intercept` und `path_coef` gesichert, um die Entwicklung der Parameter im Laufe der Optimierung zu dokumentieren.\n",
    "\n",
    "Anschließend werden die Parameter durch einen Schritt des Gradientenabstiegs angepasst. Hierbei wird die Funktion `step_gradient` aufgerufen, die die Gradienten berechnet und die Parameter in Richtung des steilsten Abstiegs der Kostenfunktion verändert. Durch die Schrittweite (`learning_rate`) wird gesteuert, wie stark die Parameter in jeder Iteration angepasst werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner(x, y, starting_intercept, starting_coef, learning_rate, num_iterations, pause_time=0.1):\n",
    "    intercept = starting_intercept\n",
    "    coef = starting_coef\n",
    "    cost_graph = []\n",
    "    path_intercept = []\n",
    "    path_coef = []\n",
    "\n",
    "    # Berechnungsschleife\n",
    "    for i in range(num_iterations):\n",
    "        # Berechnung des Fehlers\n",
    "        cost = compute_cost(intercept, coef, x, y)\n",
    "        cost_graph.append(cost)\n",
    "\n",
    "        # Speichere die aktuellen Parameter\n",
    "        path_intercept.append(intercept)\n",
    "        path_coef.append(coef)\n",
    "\n",
    "        # Aktualisiere Parameter\n",
    "        intercept, coef = step_gradient(intercept, coef, x, y, learning_rate)\n",
    "\n",
    "    # Zeichne Ergebnisse nach der Berechnung\n",
    "    plot_results(x, y, cost_graph, path_intercept, path_coef, pause_time)\n",
    "\n",
    "    return intercept, coef, cost_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(x, y, cost_graph, path_intercept, path_coef, pause_time=0.1):\n",
    "    # Initialisiere Plot\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    ax1, ax2, ax3, ax4 = axes\n",
    "\n",
    "    # Achsen für die Visualisierung vorbereiten\n",
    "    ax1.scatter(x, y, color='blue')\n",
    "    ax1.set_title(\"Datenpunkte und Regressionsgerade\")\n",
    "    ax2.set_title(\"Kostenverlauf\")\n",
    "    ax2.set_xlabel(\"Iteration\")\n",
    "    ax2.set_ylabel(\"Kosten\")\n",
    "    ax3.set_title(\"Parameter Slope/Steigung\")\n",
    "    ax3.set_xlabel(\"Iteration\")\n",
    "    ax3.set_ylabel(\"Wert\")\n",
    "    ax3.set_title(\"Parameter Intercept/y-Achsenschnittpunkt\")\n",
    "    ax3.set_xlabel(\"Iteration\")\n",
    "    ax3.set_ylabel(\"Wert\")\n",
    "\n",
    "\n",
    "    # Iteriere durch gespeicherte Werte und aktualisiere den Plot\n",
    "    for i in range(len(cost_graph)):\n",
    "        # Erhalte aktuelle Werte\n",
    "        intercept = path_intercept[i]\n",
    "        coef = path_coef[i]\n",
    "        cost = cost_graph[i]\n",
    "\n",
    "        # Erhalte aktuelle Achsengrenzen\n",
    "        x_min, x_max = ax1.get_xlim()\n",
    "        line_x = np.linspace(x_min, x_max, 100)\n",
    "        line_y = coef * line_x + intercept\n",
    "\n",
    "        # Linie zeichnen oder aktualisieren\n",
    "        if i == 0:\n",
    "            ax1.plot(line_x, line_y, color='red', label=\"Regressionsgerade\")\n",
    "        else:\n",
    "            ax1.lines[-1].set_xdata(line_x)\n",
    "            ax1.lines[-1].set_ydata(line_y)\n",
    "\n",
    "        # Update Kostenverlauf\n",
    "        ax2.clear()\n",
    "        ax2.plot(cost_graph[:i + 1], color='green', label=f\"Kosten: {cost}\")\n",
    "        ax2.set_title('Kostenverlauf')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('Kosten')\n",
    "        ax2.legend()\n",
    "\n",
    "        # Update Parameterverlauf\n",
    "        ax3.clear()\n",
    "        ax3.plot(path_coef[:i + 1], label=f\"Steigung: {coef}\", linestyle=\"-\", color=\"cyan\")\n",
    "        ax3.set_title(\"Slope\")\n",
    "        ax3.set_xlabel(\"Iteration\")\n",
    "        ax3.set_ylabel(\"Wert\")\n",
    "        ax3.legend()\n",
    "        \n",
    "        ax4.clear()\n",
    "        ax4.plot(path_intercept[:i + 1], label=f\"Intercept: {intercept}\", linestyle=\"--\", color=\"orange\")\n",
    "        ax4.set_title(\"Intercept\")\n",
    "        ax4.set_xlabel(\"Iteration\")\n",
    "        ax4.set_ylabel(\"Wert\")\n",
    "        ax4.legend()\n",
    "\n",
    "        # Anzeige aktualisieren\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "        plt.pause(pause_time)\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testdaten erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "anzahl_testdaten = 100\n",
    "x = np.random.uniform(0, 10, size=anzahl_testdaten)\n",
    "y = 2.5 * x + np.random.normal(0, 4, size=anzahl_testdaten)  # Lineare Beziehung mit Rauschen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ist die angepasste Beschreibung:\n",
    "\n",
    "---\n",
    "\n",
    "#### Gradientenverfahren starten\n",
    "Das Modell erhält hier die Startwerte für die beiden Parameter `coef` (Steigung) und `intercept` (Achsenabschnitt). Diese Parameter werden durch das Gradientenverfahren so angepasst, dass die Ziel- bzw. Verlustfunktion minimiert wird. Ziel ist es, den Fehler zwischen den tatsächlichen Werten und den Vorhersagen des Modells so gering wie möglich zu halten.\n",
    "\n",
    "Die Parameter `num_iterations` (Anzahl der Iterationen) und `learning_rate` (Schrittweite) sind sogenannte **Hyperparameter**. Sie steuern, wie das Gradientenverfahren die gesuchten Parameter bestimmt:\n",
    "- **`learning_rate`**: Gibt an, wie groß die Schritte bei der Anpassung der Parameter sein sollen. Ein zu kleiner Wert kann den Prozess verlangsamen, ein zu großer Wert kann die Konvergenz verhindern.\n",
    "- **`num_iterations`**: Legt fest, wie oft die Anpassung der Parameter durchgeführt wird. Mehr Iterationen können die Genauigkeit erhöhen, erhöhen aber auch die Rechenzeit.\n",
    "\n",
    "Am Ende des Prozesses werden die optimalen Werte für `intercept` und `coef` sowie der Verlauf der Kostenfunktion (`cost_graph`) zurückgegeben, um den Fortschritt des Verfahrens nachvollziehen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "initial_intercept = 0\n",
    "initial_coef = 0\n",
    "num_iterations = 50\n",
    "\n",
    "intercept, coef, cost_graph = gradient_descent_runner(x, y, initial_intercept, initial_coef, learning_rate, num_iterations, pause_time=0.05)\n",
    "\n",
    "print(f\"Finale Werte: Intercept = {intercept}, Coefficient = {coef}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"house_data.csv\")\n",
    "y = df.sale_price.to_numpy()\n",
    "x = df.sq_feet.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "initial_intercept = 0\n",
    "initial_coef = 0\n",
    "num_iterations = 50\n",
    "\n",
    "intercept, coef, cost_graph = gradient_descent_runner(x, y, initial_intercept, initial_coef, learning_rate, num_iterations, pause_time=0.05)\n",
    "\n",
    "print(f\"Finale Werte: Intercept = {intercept}, Coefficient = {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = (x - x.min()) / (x.max() - x.min())\n",
    "y_scaled = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "learning_rate = 0.7\n",
    "initial_intercept = 0\n",
    "initial_coef = 0\n",
    "num_iterations = 50\n",
    "\n",
    "intercept, coef, cost_graph = gradient_descent_runner(x_scaled, y_scaled, initial_intercept, initial_coef, learning_rate, num_iterations, pause_time=0.05)\n",
    "\n",
    "# Rücktransformation des Koeffizienten\n",
    "final_coef = coef * ((y.max() - y.min()) / (x.max() - x.min()))\n",
    "\n",
    "# Rücktransformation des Intercepts\n",
    "final_intercept = intercept * (y.max() - y.min()) + y.min() - (final_coef * x.min())\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(f\"Finale Werte (skalierte Werte): Intercept = {intercept:.2f}, Coefficient = {coef:.2f}\")\n",
    "print(f\"Finale Werte (ursprüngliche Skala): Intercept = {final_intercept:.2f}, Coefficient = {final_coef:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(x,y)\n",
    "x_min, x_max = plt.xlim()\n",
    "final_line_x = np.linspace(x_min, x_max, 100)  \n",
    "final_line_y = final_coef * final_line_x + final_intercept\n",
    "plt.plot(final_line_x, final_line_y, color=\"red\")\n",
    "plt.xlabel('Wohnfläche (sq_feet)')\n",
    "plt.ylabel('Verkaufspreis (sale_price)')\n",
    "plt.title('Lineare Regression auf ursprünglicher Skala')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_original = x_scaled * (x.max() - x.min()) + x.min()\n",
    "y_original = y_scaled * (y.max() - y.min()) + y.min()\n",
    "assert x.all() == x_original.all()\n",
    "assert y.all() == y_original.all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
