{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<center><h1> Pandas for Data Science</h1></center>\n",
    "<center><h2> Data Cleaning and Missing Values Management </h2></center>\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    " **Data cleaning** and **missing values management** (called **NaN** or **NA**) are two essential steps before any analysis on a database.\n",
    "\n",
    " The objective of this notebook is to detail each of these two steps in order to obtain a clean and easily usable `DataFrame`. Indeed, databases very often present this kind of problem.\n",
    "\n",
    " For this, we are going to use the `DataFrame` **`transactions`** imported in the previous exercise.\n",
    "\n",
    "\n",
    "* **(a)** Import the `pandas` module under the name `pd` and load the file `\"transactions.csv\"` in a `DataFrame` named **`transactions`**. The values in the CSV file are separated by **commas** and the column containing the identifiers is **`'transaction_id'`**.\n",
    "\n",
    "\n",
    "* **(b)** Display the first 10 rows of `transactions.csv` with the `head` method.\n",
    "\n",
    "Auf Deutsch:\n",
    "\n",
    "## Einleitung\n",
    "\n",
    " **Datenbereinigung** und **Verwaltung fehlender Werte** (**NaN** oder **NA** genannt) sind zwei wesentliche Schritte vor jeder Analyse einer Datenbank.\n",
    "\n",
    " Das Ziel dieses Notizbuches ist es, jeden dieser beiden Schritte detailliert zu beschreiben, um einen sauberen und leicht verwendbaren `DataFrame` zu erhalten. In der Tat stellen Datenbanken sehr oft diese Art von Problem dar.\n",
    "\n",
    " Dazu werden wir den `DataFrame` **`Transaktionen`** verwenden, der in der vorherigen Übung importiert wurde.\n",
    "\n",
    "\n",
    "* **(a)** Importieren Sie das Modul „pandas“ unter dem Namen „pd“ und laden Sie die Datei „transactions.csv“ in einen „DataFrame“ mit dem Namen „transactions“. Die Werte in der CSV-Datei sind durch **Kommas** getrennt und die Spalte mit den Bezeichnern ist **`'transaction_id'`**.\n",
    "\n",
    "\n",
    "**(b)** Zeigen Sie die ersten 10 Zeilen von „transactions.csv“ mit der Methode „head“ an.\n",
    "\n",
    "Übersetzt mit DeepL.com (kostenlose Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                cust_id   tran_date  prod_subcat_code  prod_cat_code  Qty  \\\n",
      "transaction_id                                                              \n",
      "80712190438      270351  28-02-2014               1.0              1   -5   \n",
      "29258453508      270384  27-02-2014               5.0              3   -5   \n",
      "51750724947      273420  24-02-2014               6.0              5   -2   \n",
      "93274880719      271509  24-02-2014              11.0              6   -3   \n",
      "51750724947      273420  23-02-2014               6.0              5   -2   \n",
      "97439039119      272357  23-02-2014               8.0              3   -2   \n",
      "45649838090      273667  22-02-2014              11.0              6   -1   \n",
      "22643667930      271489  22-02-2014              12.0              6   -1   \n",
      "79792372943      275108  22-02-2014               3.0              1   -3   \n",
      "50076728598      269014  21-02-2014               8.0              3   -4   \n",
      "\n",
      "                  Rate      Tax  total_amt Store_type  \n",
      "transaction_id                                         \n",
      "80712190438     -772.0  405.300  -4265.300     e-Shop  \n",
      "29258453508    -1497.0  785.925  -8270.925     e-Shop  \n",
      "51750724947     -791.0  166.110  -1748.110   TeleShop  \n",
      "93274880719    -1363.0  429.345  -4518.345     e-Shop  \n",
      "51750724947     -791.0  166.110  -1748.110   TeleShop  \n",
      "97439039119     -824.0  173.040  -1821.040   TeleShop  \n",
      "45649838090    -1450.0  152.250  -1602.250     e-Shop  \n",
      "22643667930    -1225.0  128.625  -1353.625   TeleShop  \n",
      "79792372943     -908.0  286.020  -3010.020        MBR  \n",
      "50076728598     -581.0  244.020  -2568.020     e-Shop  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "transactions = pd.read_csv('transactions.csv', index_col='transaction_id', sep =',')\n",
    "print(transactions.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Cleaning up a dataset\n",
    "\n",
    "> In this part we will introduce the methods of the `DataFrame` class that are essential to clean a dataset. These methods can be grouped into three different categories :\n",
    ">\n",
    ">> - **Duplicates management** (`duplicated` and `drop_duplicates` methods)\n",
    ">>\n",
    ">>\n",
    ">> - **Modification of the elements** of a `DataFrame` (`replace`, `rename` and `astype` methods)\n",
    ">>\n",
    ">>\n",
    ">> - **Operations** on the values of a `DataFrame` (`apply` method and `lambda` functions)\n",
    ">\n",
    ">\n",
    "\n",
    "### Managing duplicates (`duplicated` and ` drop_duplicates` methods)\n",
    "\n",
    "> **Duplicates** are identical entries that appear **more than once** in a dataset.\n",
    ">\n",
    "> When we first discover a dataset it is very important to **check up front** that there are no duplicates. The presence of duplicates will generate **errors** in the computation of statistics or the plotting of graphs.\n",
    ">\n",
    "> Let **`df`** be the following `DataFrame`:\n",
    ">\n",
    ">|          | Age  |Gender|  Height|\n",
    ">|----------|------|------|--------|\n",
    ">|**Robert**|  56  |   M  |   174  |\n",
    ">|**Mark**  |  23  |   M  |   182  |\n",
    ">|**Alina** |  32  |   F  |   169  |\n",
    ">|**Mark**  |  23  |   M  |   182  |\n",
    ">\n",
    "> The presence of duplicates is checked using the **`duplicated`** method of a `DataFrame`:\n",
    ">\n",
    "> ``` py\n",
    "> # We identify the rows containing duplicates\n",
    "> df.duplicated()\n",
    ">\n",
    "> >>> 0 False\n",
    "> >>> 1 False\n",
    "> >>> 2 False\n",
    "> >>> 3 True\n",
    "> ```\n",
    ">\n",
    "> This method returns  `Series` object from `pandas`, which is equivalent to the column of a `DataFrame`. The `Series` object tells us for each row wether it is a duplicate.\n",
    ">\n",
    "> In this example, the result of the `duplicated` method informs us that **the row with index 3 is a duplicate**. Indeed, it is the **exact copy** of the row with index 1.\n",
    ">\n",
    ">\n",
    ">\n",
    "> Since the `duplicated` method returns an object of the `Series` class, we can apply the **`sum`** method to it in order to count the number of duplicates:\n",
    ">\n",
    "> ``` python\n",
    "> # To calculate the sum of boolean values, we consider that True is worth 1 and False is worth 0.\n",
    "> print(df.duplicated().sum())\n",
    "> >>> 1\n",
    "> ```\n",
    ">\n",
    "> The method of the `DataFrame` class used to remove duplicates is  **`drop_duplicates`**. Its header is as follows:\n",
    ">\n",
    "> ```py\n",
    "> drop_duplicates(subset, keep, inplace)\n",
    "> ```\n",
    ">\n",
    ">> - The `subset` parameter indicates the column(s) to consider in order to identify and remove duplicates. By default, **`subset = None`** namely we consider **all** the columns of the `DataFrame`.\n",
    ">>\n",
    ">>\n",
    ">> - The `keep` parameter indicates which entry should be kept : \n",
    ">>>  *  **`'first'`** : We keep the **first** occurrence.\n",
    ">>>\n",
    ">>>\n",
    ">>>  *  **`'last'`**: We keep the **last** occurrence.\n",
    ">>>\n",
    ">>>\n",
    ">>>  *  **`False`**: We do not keep **any** occurrence.\n",
    ">>>\n",
    ">>>\n",
    ">>>  *  By default, **`keep = 'first'`**.\n",
    ">>\n",
    ">>\n",
    ">> - The **`inplace`** parameter (very common in the methods of the `DataFrame` class), specifies whether you modify **directly** the `DataFrame` (in this case `inplace = True`) or if the method returns a **copy** of the `DataFrame` (`inplace = False`). A method applied with the argument `inplace = True` is **irreversible**. By default, `inplace = False`.\n",
    ">\n",
    ">\n",
    "> <div class=\"alert alert-danger\">\n",
    "<i class=\"fa fa-info-circle\"></i>\n",
    "    You have to be very careful when using the <code>inplace</code> parameter. A good practice is to forget this parameter and assign the <code>DataFrame</code> returned by the method to a <b>new</b> <code>DataFrame</code>.\n",
    "> </div>\n",
    ">\n",
    "> The `keep` parameter is the one that is most often specified. Indeed, a database can have duplicates created on different dates. We will then specify the value of the `keep` argument to keep only the most recent entries, for example.\n",
    ">\n",
    "> Let us go back to the `df` `DataFrame` :\n",
    ">\n",
    ">|          | Age  |Gender|  Height|\n",
    ">|----------|------|------|--------|\n",
    ">|**Robert**|  56  |   M  |   174  |\n",
    ">|**Mark**  |  23  |   M  |   182  |\n",
    ">|**Alina** |  32  |   F  |   169  |\n",
    ">|**Mark**  |  23  |   M  |   182  |\n",
    ">\n",
    "> We illustrate `df` with the following illustration :\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/duplicates_en.png\" style = \"width:400px\">\n",
    ">\n",
    "> We illustrate in the following examples the entries that are **deleted** by the `drop_duplicates` method depending on the value of the `keep` parameter:\n",
    "> \n",
    "> ``` py\n",
    "> # We keep only the first occurrence of the duplicate\n",
    "> df_first = df.drop_duplicates(keep = 'first')\n",
    "> ```\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/duplicates_first_en.png\" style = \"width:400px\">\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> ``` py\n",
    "> # We keep only the last occurrence of the duplicate\n",
    "> df_last = df.drop_duplicates(keep = 'last')\n",
    "> ```\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/duplicates_last_en.png\" width=\"400\">\n",
    ">\n",
    "> ``` py\n",
    "> # We keep no duplicates\n",
    "> df_false = df.drop_duplicates(keep = False)\n",
    "> ```\n",
    "> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/duplicates_false_en.png\" width=\"400\">\n",
    ">\n",
    "\n",
    "* **(c)** How many duplicates are there in the `transactions` `DataFrame` ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUF DEUTSCH\n",
    "\n",
    "1. Bereinigung eines Datensatzes\n",
    "In diesem Abschnitt werden wir die Methoden der DataFrame-Klasse vorstellen, die für die Bereinigung eines Datensatzes unerlässlich sind. Diese Methoden können in drei verschiedene Kategorien unterteilt werden:\n",
    "\n",
    "Duplikatmanagement (duplicated und drop_duplicates Methoden)\n",
    "\n",
    "Änderung der Elemente eines DataFrame (replace, rename und astype Methoden)\n",
    "\n",
    "Operationen auf den Werten eines DataFrame (apply Methode und lambda Funktionen)\n",
    "\n",
    "Duplikate verwalten (duplicated und drop_duplicates Methoden)\n",
    "Duplikate sind identische Einträge, die mehr als einmal in einem Datensatz erscheinen.\n",
    "\n",
    "Wenn wir einen Datensatz zum ersten Mal untersuchen, ist es sehr wichtig, zuerst zu überprüfen, ob es keine Duplikate gibt. Das Vorhandensein von Duplikaten kann zu Fehlern bei der Berechnung von Statistiken oder der Darstellung von Grafiken führen.\n",
    "\n",
    "Angenommen, df ist der folgende DataFrame:\n",
    "\n",
    "Alter\tGeschlecht\tGröße\n",
    "Robert\t56\tM\t174\n",
    "Mark\t23\tM\t182\n",
    "Alina\t32\tF\t169\n",
    "Mark\t23\tM\t182\n",
    "Um das Vorhandensein von Duplikaten zu überprüfen, verwenden wir die duplicated-Methode eines DataFrame:\n",
    "\n",
    "\n",
    "# Wir identifizieren die Zeilen, die Duplikate enthalten\n",
    "df.duplicated()\n",
    "\n",
    ">>> 0 False\n",
    ">>> 1 False\n",
    ">>> 2 False\n",
    ">>> 3 True\n",
    "Diese Methode gibt ein Series-Objekt von pandas zurück, das der Spalte eines DataFrame entspricht. Das Series-Objekt zeigt uns für jede Zeile, ob es sich um ein Duplikat handelt.\n",
    "\n",
    "In diesem Beispiel informiert uns das Ergebnis der duplicated-Methode darüber, dass die Zeile mit Index 3 ein Duplikat ist. Es handelt sich dabei um eine exakte Kopie der Zeile mit Index 1.\n",
    "\n",
    "Da die duplicated-Methode ein Series-Objekt zurückgibt, können wir die sum-Methode darauf anwenden, um die Anzahl der Duplikate zu zählen:\n",
    "\n",
    "python\n",
    "Code kopieren\n",
    "# Um die Summe der booleschen Werte zu berechnen, betrachten wir True als 1 und False als 0.\n",
    "print(df.duplicated().sum())\n",
    ">>> 1\n",
    "Die Methode der DataFrame-Klasse, die verwendet wird, um Duplikate zu entfernen, ist drop_duplicates. Ihre Signatur lautet wie folgt:\n",
    "\n",
    "python\n",
    "Code kopieren\n",
    "drop_duplicates(subset, keep, inplace)\n",
    "Der Parameter subset gibt die Spalte(n) an, die verwendet werden, um Duplikate zu identifizieren und zu entfernen. Standardmäßig ist subset = None, was bedeutet, dass alle Spalten des DataFrame berücksichtigt werden.\n",
    "\n",
    "Der Parameter keep gibt an, welche Einträge behalten werden sollen:\n",
    "\n",
    "'first' : Wir behalten die erste Vorkommen.\n",
    "\n",
    "'last': Wir behalten die letzte Vorkommen.\n",
    "\n",
    "False: Wir behalten keine Vorkommen.\n",
    "\n",
    "Standardmäßig ist keep = 'first'.\n",
    "\n",
    "Der Parameter inplace (sehr häufig in den Methoden der DataFrame-Klasse) gibt an, ob der DataFrame direkt geändert wird (in diesem Fall inplace = True) oder ob die Methode eine Kopie des DataFrame zurückgibt (inplace = False). Eine Methode, die mit dem Argument inplace = True angewendet wird, ist irreversibel. Standardmäßig ist inplace = False.\n",
    "<div class=\"alert alert-danger\">\n",
    "<i class=\"fa fa-info-circle\"></i> Sie sollten beim Verwenden des Parameters <code>inplace</code> vorsichtig sein. Eine gute Praxis ist es, auf diesen Parameter zu verzichten und den zurückgegebenen <code>DataFrame</code> der Methode einer <b>neuen</b> Variablen zuzuweisen.\n",
    "\n",
    "</div>\n",
    "Der keep-Parameter wird am häufigsten angegeben. Tatsächlich kann eine Datenbank Duplikate haben, die an verschiedenen Tagen erstellt wurden. Wir geben dann den Wert des keep-Arguments an, um nur die neuesten Einträge zu behalten, zum Beispiel.\n",
    "\n",
    "Gehen wir zurück zum df DataFrame:\n",
    "\n",
    "Alter\tGeschlecht\tGröße\n",
    "Robert\t56\tM\t174\n",
    "Mark\t23\tM\t182\n",
    "Alina\t32\tF\t169\n",
    "Mark\t23\tM\t182\n",
    "Wir veranschaulichen df mit der folgenden Illustration:\n",
    "\n",
    "<br> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/duplicates_en.png\" style = \"width:400px\">\n",
    "In den folgenden Beispielen illustrieren wir die Einträge, die durch die drop_duplicates-Methode gelöscht werden, je nachdem, welchen Wert der keep-Parameter hat:\n",
    "\n",
    "python\n",
    "Code kopieren\n",
    "# Wir behalten nur das erste Vorkommen des Duplikats\n",
    "df_first = df.drop_duplicates(keep = 'first')\n",
    "<br> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/duplicates_first_en.png\" style = \"width:400px\"> <br>\n",
    "python\n",
    "Code kopieren\n",
    "# Wir behalten nur das letzte Vorkommen des Duplikats\n",
    "df_last = df.drop_duplicates(keep = 'last')\n",
    "<img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/duplicates_last_en.png\" width=\"400\">\n",
    "python\n",
    "Code kopieren\n",
    "# Wir behalten keine Duplikate\n",
    "df_false = df.drop_duplicates(keep = False)\n",
    "<img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/duplicates_false_en.png\" width=\"400\">\n",
    "\n",
    "\n",
    "(c) Wie viele Duplikate gibt es im transactions DataFrame?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(112)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anzahl_dup=transactions.duplicated().sum()\n",
    "anzahl_dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transactions were recorded in anti-chronological order, i.e. the **first rows** contain the most **recent** transactions and the last rows the oldest transactions.\n",
    "\n",
    "* **(d)** Eliminate duplicates from the database by keeping only the first occurrence, i.e. the most recent transaction.\n",
    "\n",
    "\n",
    "* **(e)** Using the **`subset`** and **`keep`** parameters of the `drop_duplicates` method of `transactions`, display the **most recent** transaction for **each category of `prod_cat_code`**. To do this, you can remove all the duplicates from the `prod_cat_code` column by keeping only the first occurrence.\n",
    "\n",
    "AUF DEUTSCH\n",
    "\n",
    "Die Transaktionen wurden in antichronologischer Reihenfolge erfasst, d. h. die **ersten Zeilen** enthalten die **jüngsten** Transaktionen und die letzten Zeilen die ältesten Transaktionen.\n",
    "\n",
    "* **(d)** Eliminierung von Duplikaten aus der Datenbank, indem nur das erste Vorkommen, d.h. die jüngste Transaktion, beibehalten wird.\n",
    "\n",
    "\n",
    "* **(e)** Mit den Parametern **`subset`** und **`keep`** der Methode `drop_duplicates` von `transactions`, zeigen Sie die **jüngste** Transaktion für **jede Kategorie von `prod_cat_code`** an. Zu diesem Zweck können Sie alle Duplikate aus der Spalte „prod_cat_code“ entfernen, indem Sie nur das erste Vorkommen behalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Duplikate: 112\n",
      "                cust_id   tran_date  prod_subcat_code  prod_cat_code  Qty  \\\n",
      "transaction_id                                                              \n",
      "80712190438      270351  28-02-2014               1.0              1   -5   \n",
      "29258453508      270384  27-02-2014               5.0              3   -5   \n",
      "51750724947      273420  24-02-2014               6.0              5   -2   \n",
      "\n",
      "                  Rate      Tax  total_amt Store_type  \n",
      "transaction_id                                         \n",
      "80712190438     -772.0  405.300  -4265.300     e-Shop  \n",
      "29258453508    -1497.0  785.925  -8270.925     e-Shop  \n",
      "51750724947     -791.0  166.110  -1748.110   TeleShop  \n",
      "Anzahl der Duplikate bei Spalte prod_cat_code: 23047\n",
      "Nach Löschen: 0\n"
     ]
    }
   ],
   "source": [
    "# Berechnen der Duplikaten in einer bestimmten Spalte\n",
    "duplikate_count_spalten = transactions.duplicated().sum()\n",
    "print(f\"Anzahl der Duplikate: {duplikate_count_spalten}\")\n",
    "print(transactions.head(3))\n",
    "\n",
    "duplikate_count_spalten = transactions.duplicated(subset = 'prod_cat_code').sum()\n",
    "print(f\"Anzahl der Duplikate bei Spalte prod_cat_code: {duplikate_count_spalten}\")\n",
    "\n",
    "#Löschen der Duplikate:\n",
    "transactions = transactions.drop_duplicates(subset='prod_cat_code')\n",
    "\n",
    "#Überprüfen, wie viele Duplikate bleibt noch:\n",
    "duplikate_count_spalten = transactions.duplicated(subset='prod_cat_code').sum()\n",
    "print(f'Nach Löschen: {duplikate_count_spalten}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Modification of the elements of a `DataFrame` (` replace`, `rename` and `astype` methods)\n",
    "\n",
    "\n",
    "> The **`replace`** method allows to **replace** one or more values ​​of a column of a` DataFrame`.\n",
    ">\n",
    "> Its header is as follows:\n",
    ">\n",
    "> ``` python\n",
    "> replace(to_replace, value, ...)\n",
    "> ```\n",
    ">\n",
    ">> - The `to_replace` parameter contains the value or the list of values **to be replaced**. It can be a list of integers, strings, booleans, etc.\n",
    ">>\n",
    ">>\n",
    ">> - The `value` parameter contains the value or the list of the substitute **values**. It can also be a list of integers, strings, booleans, etc.\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/replace_en.png\" height=\"400px\">\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> In addition to modifying the elements of a `DataFrame`, it is possible to **rename** its columns.\n",
    ">\n",
    "> This is possible thanks to the **`rename`** method which takes as argument a **dictionary** whose **keys** are the **old** names and the **values** are the **new** names. You must also fill in the argument **`axis = 1`** to specify that the names to rename are those of the columns.\n",
    ">\n",
    "> ``` py\n",
    "> # Creation of the dictionary associating the old names with the new column names\n",
    "> dictionary = {'old_name1': 'new_name1',\n",
    ">               'old_name2': 'new_name2'}\n",
    ">\n",
    "> # We rename the variables using the rename method\n",
    "> df = df.rename(dictionary, axis = 1)\n",
    "> ```\n",
    ">\n",
    ">\n",
    ">\n",
    "> It is sometimes necessary to modify not only the name of a column but also its **type**.\n",
    ">\n",
    "> For example, it is possible that when importing a database, a variable is of type string when in fact it is a numerical variable. Whenever one of the entries in the column is incorrectly recognized, `pandas` will consider that this column is of type string.\n",
    ">\n",
    "> This is possible thanks to the **`astype`** method.\n",
    ">\n",
    "> The types that we will see most often are:\n",
    ">\n",
    ">>  * `str`: Character string (`'Hello'`).\n",
    ">>  * `float`: Floating point number (`1.0`, `1.14123`).\n",
    ">>  * `Int`: Integer (`1`,`1231`)\n",
    ">\n",
    ">\n",
    "> As for the **`rename`** method, **`astype`** can take as argument a dictionary whose **keys** are the **names of the columns whose type should be modified** and the **values** are the **new types** to assign. This is useful if you want to change the type of several columns at once.\n",
    ">\n",
    ">\n",
    "> Most often, we will directly select the column whose type should be modified and overwrite it by applying the **`astype`** method to it.\n",
    ">\n",
    "> ``` python\n",
    "> # Method 1: Creation of a dictionary then call to the astype method of the DataFrame\n",
    "> dictionary = {'col_1': 'int',\n",
    ">               'col_2': 'float'}\n",
    "> df = df.astype(dictionary)\n",
    ">\n",
    "> # Method 2: Selection of the column and then calling the astype method of a Series\n",
    "> df['col_1'] = df['col_1'].astype('int')\n",
    "> ```\n",
    ">\n",
    ">\n",
    "> <div class='alert alert-success'>\n",
    "<i class='fa fa-exclamation-circle'></i>\n",
    "    These methods also have the <code>inplace</code> parameter to perform the operation directly on the <code>DataFrame</code>. To be used with great caution.</div>\n",
    "\n",
    "\n",
    "* If you make a mistake in the next exercise, you can re-import and redo the preprocessing by running the following cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "transactions = pd.read_csv(\"transactions.csv\", sep = ',', index_col = \"transaction_id\")\n",
    "\n",
    "# Removal of duplicates\n",
    "transactions = transactions.drop_duplicates(keep = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **(f)** Import the `numpy` module under the name` np`.\n",
    "\n",
    "\n",
    "\n",
    "* **(g)** Replace the modalities **`['e-Shop', 'TeleShop', 'MBR', 'Flagship store', np.nan]`** of the **`Store_type`** column by the modalities **`[1, 2, 3, 4, 0]`**.\n",
    "The `np.nan` value is the one that encodes a missing value. We will replace this value with `0`.\n",
    "\n",
    "\n",
    "\n",
    "* **(h)** Convert the type of the columns **`Store_type`** and **`prod_subcat_code`** to type **`'int'`**.\n",
    "\n",
    "\n",
    "\n",
    "* **(i)** Rename the `'Store_type`',`'Qty'`, `'Rate'` and `'Tax'` columns with `'store_type'`,`'qty'`, `'rate'` and `'tax'`.\n",
    "\n",
    "\n",
    "Auf Deutsch:\n",
    " \n",
    "**(f)** Importieren Sie das Modul „numpy“ unter dem Namen „np“.\n",
    "\n",
    "\n",
    "\n",
    "* **(g)** Ersetzen Sie die Modalitäten **`['e-Shop', 'TeleShop', 'MBR', 'Flagship store', np.nan]`** der Spalte **`Store_type`** durch die Modalitäten **`[1, 2, 3, 4, 0]`**.\n",
    "Der Wert `np.nan` ist derjenige, der einen fehlenden Wert kodiert. Wir werden diesen Wert durch `0` ersetzen.\n",
    "\n",
    "\n",
    "\n",
    "* **(h)** Konvertieren Sie den Typ der Spalten **`Store_type`** und **`prod_subcat_code`** in den Typ **`'int'`**.\n",
    "\n",
    "Übersetzt mit DeepL.com (kostenlose Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>tran_date</th>\n",
       "      <th>prod_subcat_code</th>\n",
       "      <th>prod_cat_code</th>\n",
       "      <th>Qty</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Tax</th>\n",
       "      <th>total_amt</th>\n",
       "      <th>Store_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transaction_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80712190438</th>\n",
       "      <td>270351</td>\n",
       "      <td>28-02-2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-5</td>\n",
       "      <td>-772.0</td>\n",
       "      <td>405.300</td>\n",
       "      <td>-4265.300</td>\n",
       "      <td>e-Shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29258453508</th>\n",
       "      <td>270384</td>\n",
       "      <td>27-02-2014</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-5</td>\n",
       "      <td>-1497.0</td>\n",
       "      <td>785.925</td>\n",
       "      <td>-8270.925</td>\n",
       "      <td>e-Shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51750724947</th>\n",
       "      <td>273420</td>\n",
       "      <td>24-02-2014</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-2</td>\n",
       "      <td>-791.0</td>\n",
       "      <td>166.110</td>\n",
       "      <td>-1748.110</td>\n",
       "      <td>TeleShop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                cust_id   tran_date  prod_subcat_code  prod_cat_code  Qty  \\\n",
       "transaction_id                                                              \n",
       "80712190438      270351  28-02-2014               1.0              1   -5   \n",
       "29258453508      270384  27-02-2014               5.0              3   -5   \n",
       "51750724947      273420  24-02-2014               6.0              5   -2   \n",
       "\n",
       "                  Rate      Tax  total_amt Store_type  \n",
       "transaction_id                                         \n",
       "80712190438     -772.0  405.300  -4265.300     e-Shop  \n",
       "29258453508    -1497.0  785.925  -8270.925     e-Shop  \n",
       "51750724947     -791.0  166.110  -1748.110   TeleShop  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "transactions.head(3)\n",
    "#transactions.dtype{'Store_type':int, 'prod_subcat_code': int}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Operations on the values ​​of a `DataFrame` (`apply` method and `lambda` functions)\n",
    "\n",
    "> It is often interesting to modify or aggregate the information of the columns of a `DataFrame` using an operation or a function.\n",
    ">\n",
    "> These operations can be any type of function **which takes a column** as argument. Thus, the **numpy module is perfectly suited** to perform operations on this type of object.\n",
    ">\n",
    "> The method used to perform an operation on a column is the **`apply`** method of a `DataFrame` whose header is:\n",
    ">\n",
    "> ``` python\n",
    "> apply(func, axis, ...)\n",
    "> ```\n",
    ">\n",
    "> where:\n",
    ">> * **`func`** is the function to apply to the column.\n",
    ">> * **`axis`** is the dimension on which the operation must be applied.\n",
    ">\n",
    "> <span style=\"color:#09b038; text-decoration : underline\"> Example: </span> `apply` and `np.sum`\n",
    ">\n",
    "> For each column with numerical values, we want to calculate the **sum of all rows**. The `sum` function of `numpy` does this, so we can use it with the `apply` method.\n",
    ">\n",
    "> Since we are going to perform an operation on the **rows**, we must therefore specify the argument **`axis = 0`** in the `apply` method.\n",
    ">\n",
    "> ``` py\n",
    "> # Sum of the ROWS for each column of df\n",
    "> df_lines = df.apply(np.sum, axis = 0)\n",
    "> ```\n",
    "> The result is the following:\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/apply_sum_lines_en.png\" style = 'height:300px'>\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> Now, for each row we want to compute the **sum of all the columns**.\n",
    ">\n",
    "> We are going to perform this operation on the columns, we must therefore specify the argument **`axis = 1`** in the `apply` method.\n",
    ">\n",
    "> ``` py\n",
    "> # Sum of columns for each ROW of df\n",
    "> df_columns = df.apply(np.sum, axis = 1)\n",
    "> ```\n",
    ">\n",
    "> The result is the following:\n",
    "> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/apply_sum_columns_en.png\" style =\"height:280px\">\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> These examples only illustrate the use of the `apply` method. To actually compute the sum of rows or columns, it is better to use the **`sum`** method of a `DataFrame` or a `Series`, which behaves in exactly the same way as the `sum` method of a numpy array.\n",
    "\n",
    "The `tran_date` column of `transactions` contains the dates of the transactions in the format **`('day-month-year')`** (ex: `'28-02-2014'`). The dates are of type string: it is not possible to perform statistics on this variable for the moment.\n",
    "\n",
    "We would rather have **3 different columns** for the day, month and year of each transaction. This would allow us, for example, to analyze and detect trends in transaction dates.\n",
    "\n",
    "\n",
    "The date `'28-02-2014'` is a string. The day, month and year are separated by a hyphen **`'-'`**. The character string class has the **`split`** method to split a string on a specific character:\n",
    "\n",
    "``` python\n",
    "date = '28-02-2014 '\n",
    "\n",
    "# Splitting the string on the '-' character\n",
    "print(date.split('-'))\n",
    ">>> ['28', '02', '2014']\n",
    "```\n",
    "\n",
    "This method returns a **list** containing the slices of the string on the specified character. Thus, to retrieve the day, all you have to do is select the **first** element of the split. To recover the month, we must take the second element and for the year the third.\n",
    "\n",
    "* **(j)** Define a function **`get_day`** taking as argument a string and which returns the first element of its split by the character `'-'`.\n",
    "\n",
    "\n",
    "* **(k)** Define the functions **`get_month`** and **`get_year`** which do the same with the second and third element of the split.\n",
    "\n",
    "\n",
    "* **(l)** In 3 variables called **`days`**, **`months`** and **`years`**, store the result of the **`apply`** method on the **`tran_date`** column with the `get_day`, `get_month` and `get_year` functions. As these functions work element-wise, it is not necessary to specify the argument **`axis`** in the `apply` method.\n",
    "\n",
    "\n",
    "\n",
    "* **(m)** Create the columns **`'day'`**, **`'month'`** and **`'year'`** in the `transactions` `DataFrame` and store the values of the `days`, `months` and `years`. Creating a new column is simply done by declaring it:\n",
    ">\n",
    ">``` python\n",
    "># Create a new column 'day' with the values contained in days.\n",
    ">transactions['day'] = days\n",
    ">```\n",
    "\n",
    "\n",
    "* **(n)** Display the first 5 rows of `transactions`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_id</th>\n",
       "      <th>tran_date</th>\n",
       "      <th>prod_subcat_code</th>\n",
       "      <th>prod_cat_code</th>\n",
       "      <th>Qty</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Tax</th>\n",
       "      <th>total_amt</th>\n",
       "      <th>Store_type</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transaction_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80712190438</th>\n",
       "      <td>270351</td>\n",
       "      <td>28-02-2014</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-5</td>\n",
       "      <td>-772.0</td>\n",
       "      <td>405.300</td>\n",
       "      <td>-4265.300</td>\n",
       "      <td>e-Shop</td>\n",
       "      <td>28</td>\n",
       "      <td>02</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29258453508</th>\n",
       "      <td>270384</td>\n",
       "      <td>27-02-2014</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-5</td>\n",
       "      <td>-1497.0</td>\n",
       "      <td>785.925</td>\n",
       "      <td>-8270.925</td>\n",
       "      <td>e-Shop</td>\n",
       "      <td>27</td>\n",
       "      <td>02</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51750724947</th>\n",
       "      <td>273420</td>\n",
       "      <td>24-02-2014</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-2</td>\n",
       "      <td>-791.0</td>\n",
       "      <td>166.110</td>\n",
       "      <td>-1748.110</td>\n",
       "      <td>TeleShop</td>\n",
       "      <td>24</td>\n",
       "      <td>02</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93274880719</th>\n",
       "      <td>271509</td>\n",
       "      <td>24-02-2014</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1363.0</td>\n",
       "      <td>429.345</td>\n",
       "      <td>-4518.345</td>\n",
       "      <td>e-Shop</td>\n",
       "      <td>24</td>\n",
       "      <td>02</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51750724947</th>\n",
       "      <td>273420</td>\n",
       "      <td>23-02-2014</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-2</td>\n",
       "      <td>-791.0</td>\n",
       "      <td>166.110</td>\n",
       "      <td>-1748.110</td>\n",
       "      <td>TeleShop</td>\n",
       "      <td>23</td>\n",
       "      <td>02</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                cust_id   tran_date  prod_subcat_code  prod_cat_code  Qty  \\\n",
       "transaction_id                                                              \n",
       "80712190438      270351  28-02-2014               1.0              1   -5   \n",
       "29258453508      270384  27-02-2014               5.0              3   -5   \n",
       "51750724947      273420  24-02-2014               6.0              5   -2   \n",
       "93274880719      271509  24-02-2014              11.0              6   -3   \n",
       "51750724947      273420  23-02-2014               6.0              5   -2   \n",
       "\n",
       "                  Rate      Tax  total_amt Store_type day month  year  \n",
       "transaction_id                                                         \n",
       "80712190438     -772.0  405.300  -4265.300     e-Shop  28    02  2014  \n",
       "29258453508    -1497.0  785.925  -8270.925     e-Shop  27    02  2014  \n",
       "51750724947     -791.0  166.110  -1748.110   TeleShop  24    02  2014  \n",
       "93274880719    -1363.0  429.345  -4518.345     e-Shop  24    02  2014  \n",
       "51750724947     -791.0  166.110  -1748.110   TeleShop  23    02  2014  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_day(tag):\n",
    "    return tag.split('-')[0]\n",
    "\n",
    "def get_month(monat):\n",
    "    return monat.split('-')[1]\n",
    "\n",
    "def get_year(jahr):\n",
    "    return jahr.split('-')[2]\n",
    "\n",
    "\n",
    "days = transactions['tran_date'].apply(get_day)\n",
    "months = transactions['tran_date'].apply(get_month)\n",
    "years = transactions['tran_date'].apply(get_year)\n",
    "\n",
    "#Neue Spalten:\n",
    "transactions['day'] = days\n",
    "transactions['month'] = months\n",
    "transactions['year'] = years\n",
    "\n",
    "transactions.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> The **`apply`** method is very powerful when combined with a **`lambda`** function.\n",
    ">\n",
    "> In Python, the keyword **`lambda`** is used to define an **anonymous** function: a function declared without name.\n",
    ">\n",
    "> A function **`lambda`** can take any number of arguments, but can only have one expression.\n",
    ">\n",
    "> Here is its syntax: \n",
    ">\n",
    "> ```python \n",
    "> lambda arguments: expression\n",
    "> ```\n",
    ">\n",
    "> `Lambda` functions allow you to define functions with a very short syntax : \n",
    ">\n",
    "> \n",
    ">```python\n",
    "># Example 1 \n",
    "> x = lambda a: a + 2\n",
    ">print(x(3))\n",
    "> >>> 5\n",
    ">```\n",
    ">```python\n",
    "># Example 2 \n",
    "> x = lambda a, b : a * b\n",
    "> print(x(2, 3))\n",
    "> >>> 6\n",
    ">```\n",
    ">```python\n",
    "># Example 3 \n",
    ">x = lambda a, b, c : a - b + c\n",
    ">print(x(1, 2, 3))\n",
    "> >>> 2\n",
    ">```\n",
    ">Although syntactically different, **`lambda`** functions behave in the same way as regular functions that are declared using the **`def`** keyword.\n",
    ">\n",
    "> The classic definition of a function is done with the **`def`** keyword:\n",
    "> ``` py\n",
    "> def increment(x):\n",
    ">     return x + 1\n",
    "> ```\n",
    ">\n",
    "> It is also possible to define a function with the keyword **`lambda`**:\n",
    "> ``` py\n",
    "> increment = lambda x: x + 1\n",
    "> ```\n",
    ">\n",
    "> The first method is very clean but the advantage of the second is that it can be defined on-the-fly directly **within** the **`apply`** method.\n",
    ">\n",
    "> Thus, the previous exercise can be done with a very compact syntax:\n",
    ">\n",
    "> ``` python\n",
    "> transactions['day'] = transactions['tran_date'].apply(lambda date: date.split('-')[0])\n",
    "> ```\n",
    ">\n",
    "> This kind of syntax is very practical and very often used for cleaning databases.\n",
    ">\n",
    ">\n",
    ">The `prod_subcat_code` column of `transactions` depends on the `prod_cat_code` column because it identifies a **subcategory** of product. It would make more sense to have the category and subcategory of a product in the same variable.\n",
    ">\n",
    ">To do this, we will merge the values of these two columns:\n",
    ">\n",
    ">> * We will first convert the values of these two columns into strings using the **`astype`** method.\n",
    ">>\n",
    ">>\n",
    ">>* Then, we will concatenate these strings to have a unique code representing both the category and sub-category. This can be done in the following way:\n",
    ">>\n",
    ">> ``` python\n",
    ">> string1 = \"I think\"\n",
    ">> string2 = \"therefore I am.\"\n",
    ">>\n",
    ">> # Concatenation of the two strings by separating them with a space\n",
    ">> print (string1 + \" \" + string2)\n",
    ">> >>> I think therefore I am.\n",
    ">> ```\n",
    "\n",
    "To apply a lambda function to an entire row, you must specify the argument **`axis = 1`** in the `apply` method. In the function itself, the columns of the row can be accessed as on a `DataFrame`:\n",
    "\n",
    "``` python\n",
    "# Computation of the unit price of a product\n",
    "transactions.apply(lambda row: row['total_ amt']/row['qty'], axis = 1)\n",
    "```\n",
    "\n",
    "* **(o)** Using a `lambda` function applied to `transactions`, create a column **`'prod_cat'`** in `transactions` containing the concatenation of the values of` prod_cat_code` and `prod_subcat_code` separated by a hyphen `'-'`. Remember to convert the values to strings.\n",
    "\n",
    ">Displaying this column should yield:\n",
    ">\n",
    ">```\n",
    ">transaction_id\n",
    ">80712190438     1-1\n",
    ">29258453508     3-5\n",
    ">51750724947     5-6\n",
    ">93274880719     6-11\n",
    ">51750724947     5-6\n",
    ">                ...\n",
    ">94340757522     5-12\n",
    ">89780862956     1-4\n",
    ">85115299378     6-2\n",
    ">72870271171     5-11\n",
    ">77960931771     5-11\n",
    ">```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               prod_cat\n",
      "transaction_id         \n",
      "80712190438       1-1.0\n",
      "29258453508       3-5.0\n",
      "51750724947       5-6.0\n",
      "93274880719      6-11.0\n",
      "51750724947       5-6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transactions['prod_cat'] = transactions.apply(lambda row: str(row['prod_cat_code']) + '-' + str(row['prod_subcat_code']), axis=1)\n",
    "\n",
    "print(transactions[['prod_cat']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Dealing with missing values\n",
    ">\n",
    "> A **missing value** is either:\n",
    ">> * An unspecified value.\n",
    ">> * A value that does not exist. In general, they result from mathematical calculations having no solution (a division by zero for example).\n",
    ">\n",
    "> A missing value appears under the name **NaN** (\"**N**ot **a** **N**umber\") in a `DataFrame`.\n",
    ">\n",
    "> In this part, we will see several methods to:\n",
    ">\n",
    ">> - **Detect** missing values (`isna` and `any` methods)\n",
    ">> - **Replace** these values (`fillna` method)\n",
    ">> - **Delete** missing values (`dropna` method)\n",
    ">\n",
    "> In one of the previous exercises, we used the `replace` method of `transactions` to replace missing values with `0`. This approach is not rigorous and should not be done in practice.\n",
    ">\n",
    "> For this reason, we are going to re-import the raw version of `transactions` to undo the steps we did in the previous exercises.\n",
    "\n",
    "* **(a)** Run the cell below to re-import `transactions`, remove duplicates and rename its columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transachtions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtransachtions\u001b[49m\u001b[38;5;241m.\u001b[39misna()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transachtions' is not defined"
     ]
    }
   ],
   "source": [
    "transachtions.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Detecting missing values  (`isna` and `any` methods)\n",
    ">\n",
    "> The **`isna`** method of a `DataFrame` detects its missing values. This method does not take any arguments.\n",
    ">\n",
    "> This method returns the same `DataFrame` whose values are:\n",
    ">> * **`True`** if the original table cell is a missing value (`np.nan`)\n",
    ">> * **`False`** otherwise.\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/is_null_en.png\" width=\"750\">\n",
    ">\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> Since the `isna` method returns a `DataFrame`, we can use it with other methods of the `DataFrame` class to get more precise information:\n",
    ">\n",
    ">> - The **`any`** method - thanks to its `axis` argument - allows to determine **which columns** (`axis = 0`) or **which rows** (`axis = 1`) contain at least one missing value.\n",
    ">>\n",
    ">>\n",
    ">> - The **`sum`** method counts the number of missing values per column or row (by specifying the `axis` argument). It is possible to use other statistical methods like `mean`,` max`, `argmax`, etc.\n",
    ">\n",
    ">\n",
    "> Here are many examples of using the `any` and `sum` methods with `isna`:\n",
    ">\n",
    "> We use the `DataFrame` **`df`** from the previous illustrations:\n",
    ">\n",
    "> |    | Name    | Country   |   Age |\n",
    "> |---:|:--------|:----------|------:|\n",
    "> |  0 | NaN     | Australia |   NaN |\n",
    "> |  1 | Duchamp | France    |    25 |\n",
    "> |  2 | Hana    | Japan     |    54 |\n",
    ">\n",
    "> The `df.isna()` instruction returns:\n",
    ">\n",
    ">\n",
    "> |    |   Name  |Country |   Age |\n",
    "> |---:|--------:|-------:|------:|\n",
    "> |  0 |   True  | False  | True  |\n",
    "> |  1 |   False | False  | False |\n",
    "> |  2 |   False | False  | False |\n",
    ">\n",
    "> ``` python\n",
    "> # COLUMNS containing at least one missing value are detected\n",
    "> df.isna().any(axis = 0)\n",
    "> ```\n",
    "> ```\n",
    "> >>> Name     True\n",
    "> >>> Country  False\n",
    "> >>> Age      True\n",
    "> ```\n",
    ">\n",
    "> ``` python\n",
    "> # ROWS containing at least one missing value are detected\n",
    "> df.isna().any (axis = 1)\n",
    "> ```\n",
    ">```\n",
    "> >>> 0    True\n",
    "> >>> 1    False\n",
    "> >>> 2    False\n",
    "> ````\n",
    ">\n",
    "> ``` python\n",
    "> # Using conditional indexing to display entries\n",
    "> # containing missing values\n",
    "> df[df.isna().any(axis = 1)]\n",
    "> ```\n",
    ">\n",
    ">\n",
    "> which returns the `DataFrame`:\n",
    ">\n",
    "> |    |   Name| Country   |   Age |\n",
    "> |---:|------:|:----------|------:|\n",
    "> |  0 |   NaN | Australia |   NaN |\n",
    ">\n",
    "> ``` python\n",
    "> # We count the number of missing values for each COLUMN\n",
    "> df.isnull().sum(axis = 0)\n",
    "> ```\n",
    "> ```\n",
    "> >>> Name     1\n",
    "> >>> Country  0\n",
    "> >>> Age      1\n",
    "> ```\n",
    ">\n",
    "> ``` python\n",
    "> # Count the number of missing values for each ROW\n",
    "> df.isnull().sum(axis = 1)\n",
    "> ```\n",
    "> ```\n",
    "> >>> 0   2\n",
    "> >>> 1   0\n",
    "> >>> 2   0\n",
    "> ```\n",
    "> The methods `isna`and `isnull`have exactly the same behavior.\n",
    "\n",
    "* **(b)** How many columns of the `transactions` `DataFrame` contain missing values?\n",
    "\n",
    "\n",
    "\n",
    "* **(c)** How many of `transactions`' entries contain missing values? You can use the `any` method along with the `sum` method.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **(d)** Which column of `transactions` contains **the most** of missing values? You can use the `idxmax` method that returns the index of first occurrence of maximum over the requested axis.\n",
    "\n",
    "\n",
    "\n",
    "* **(e)** Show `transaction` entries that contain at least one missing value in the `'rate'`, `'tax'` and `'total_amt'` columns. What do you notice?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Replacing missing values (`fillna` method)\n",
    "\n",
    "> The `fillna` method allows you to replace the missing values of a `DataFrame` by **values you want**.\n",
    ">\n",
    "> ``` python\n",
    "> # We replace all the NaNs of the DataFrame by zeros\n",
    "> df.fillna(0)\n",
    ">\n",
    "> # We replace the NaNs of each numerical column by the average on this column\n",
    "> df.fillna(df.mean()) # df.mean() can be replaced by any statistical method.\n",
    "> ```\n",
    ">\n",
    "> It is common to replace missing values of a column containing **numerical** values with **statistics** like:\n",
    ">> * The **mean**: `.mean`\n",
    ">> * The **median**: `.median`\n",
    ">> * The **minimum / maximum**: `.min` / `.max`.\n",
    ">\n",
    "> For categorical type columns, replace the missing values with:\n",
    ">> * The **mode**, i.e. the most frequent modality: `.mode`.\n",
    ">> * A **constant** or arbitrary category: `0`,` -1`.\n",
    ">\n",
    "> To avoid making replacement errors, it is very important to **select the correct columns** before using the `fillna` method.\n",
    "\n",
    "If you make mistakes in the following exercise, you can re-import `transactions` using the following cell:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "transactions = pd.read_csv(\"transactions.csv\", sep = ',', index_col = \"transaction_id\")\n",
    "\n",
    "# Removal of duplicates\n",
    "transactions = transactions.drop_duplicates(keep = 'first')\n",
    "\n",
    "# Renaming the columns\n",
    "new_names = {'Store_type' : 'store_type',\n",
    "              'Qty'       : 'qty',\n",
    "              'Rate'      : 'rate',\n",
    "              'Tax'       : 'tax'}\n",
    "\n",
    "transactions = transactions.rename(new_names, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **(f)** Replace the missing values in **`prod_subcat_code`**  column of `transactions` with `-1`.\n",
    "\n",
    "\n",
    "* **(g)** Determine **the most frequent modality** (the mode) of the **`store_type`** column of `transactions`.\n",
    "\n",
    "\n",
    "* **(h)** Replace the missing values of the `store_type` column by this modality. The value of this modality is accessed **at index 0** of the `Series` returned by `mode`.\n",
    "\n",
    "\n",
    "* **(i)** Check that the `prod_subcat_code` and `store_type` columns of `transactions` no longer contain missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Removing missing values (`dropna` method)\n",
    ">\n",
    "> The `dropna` method allows you to remove rows or columns containing missing values.\n",
    ">\n",
    "> The header of the method is as follows:\n",
    ">\n",
    "> ```py \n",
    "> dropna(axis, how, subset, ..)\n",
    "> ```\n",
    ">\n",
    ">> - The **`axis`** parameter specifies whether to delete rows or columns (**`0`** for rows, **`1`** for columns).\n",
    ">>\n",
    ">>\n",
    ">> - The **`how`** parameter lets you specify how the rows (or columns) are deleted:\n",
    ">>>    * **`how = 'any'`**: We delete the row (or column) if it contains **at least one** missing value.\n",
    ">>>    * **`how = 'all'`**: We delete the row (or column) if it contains **only** missing values.\n",
    ">>\n",
    ">>\n",
    ">> - The **`subset`** parameter is used to specify the columns/rows on which the search for missing values is carried out.\n",
    ">\n",
    ">\n",
    "> <span style=\"color:#09b038; text-decoration : underline\"> Example: </span><br>\n",
    ">\n",
    "> ``` python\n",
    "> # We delete all the rows containing at least one missing value\n",
    "> df = df.dropna(axis = 0, how = 'any')\n",
    ">\n",
    "> # We delete the empty columns\n",
    "> df = df.dropna(axis = 1, how = 'all')\n",
    ">\n",
    "> # We remove the rows with missing values in the 3 columns 'col2', 'col3' and 'col4'\n",
    "> df.dropna(axis = 0, how = 'all', subset = ['col2', 'col3', 'col4'])\n",
    "> ```\n",
    ">\n",
    "> As with the other methods of replacing values of a `DataFrame`, the `inplace` argument can be used with great care to perform the modification directly without reassignment.\n",
    "\n",
    "\n",
    "Transaction data for which the transaction amount is not provided is of no interest to us. For this reason:\n",
    "\n",
    "* **(j)** Delete the `transaction` entries for which the **`rate`**, **`tax`** and **`total_amt`** columns are **all** empty.\n",
    "\n",
    "\n",
    "* **(k)** Check that the columns of `transactions` **no longer contain missing values**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
